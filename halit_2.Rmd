---
title: "52414 - lab 2 "
author: "Yoray Mauda 315874404 and Halit Cohen 318356854"
date: "09/06/2022"
output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```  

### Question 1

In this question, we will wrangle and scrape data from the book "Moby Dick", extract information, print text, make numerous manipulation on the text, and also create different distributions for it.

###### A

First, we will fetch the HTML, and then we will print the first line.

```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
# Read the html into r:
webpage <- read_html(url)
```  

```{r}
# In line 35, We are creating the first paragraph, and then extract the first line out of it. In line 36 we're returning the sentence.
first_sentence <- webpage %>% html_nodes("div") %>% html_text()
first_sentence[1]
```

##### B

```{r}
words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
words_html <- words_html[[1]]
words_html <- words_html[words_html!=""]
#count words
print(length(words_html))
# length words
words_n <-nchar(words_html)
#longest word, mean, median
max(words_n)
mean(words_n)
median(words_n)
# Create frequency table
freq_words <- sort(table(unlist(words_html)),
               decreasing = TRUE)
#most common word
mcw <-which.max(freq_words)
mcw
# the distribution of word 
tab_n<-table(str_length(words_html))/length(words_html)
barplot(tab_n,main="word length Distribution")
```

###c
```{r}
sort(table(words_html), decreasing = TRUE)[1:10]
```
Not surprisingly, the most common words are conjunction or body pronouns.  

### Question 2

###### A

In this section we will divide the text into chapters, and will plot the count of the number of words per each episode.

```{r}
moby_text <- html_text2(webpage)
```

To discard the beginning until the EXTRACT section, We will use `str_split` and remove the beginning.

```{r}
cut_text <- str_split(moby_text, "\rErromangoan.\t\r\n\r H2 anchor\r\n\r\n\n\n\n\n\r\n\r ")[[1]][2]
```

We will not print the results we have received because it is long, but we have notices that between chapters we have division that looks like `\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r `. Thus, we will split our text based on this division.

```{r}
chapters <- strsplit(cut_text, "\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r ")
chapters <- chapters[[1]]
```

Now, for plotting the data, we will loop over the chapters and extract the wanted data for our plot

```{r}
  x_chapter_num <- c()
  y_words_per_chapter <- c()
  for (i in 1:length(chapters)){
    fixed_chapter_i <- str_replace_all(chapters[i], "\\s+", " ")
    splitsplat <- str_split(fixed_chapter_i, " ")
    splitsplat <- c(splitsplat)
    splitsplat <- splitsplat[[1]]
    x_chapter_num[i] <- i
    y_words_per_chapter[i] <- length(splitsplat)
  }
barplot(y_words_per_chapter, main = "Number of Words Per Chapter",
        xlab = "Chapter Number", ylab = "Words Number", names.arg = x_chapter_num,
        col = "darkred")
```
###### B

For this section we will create a function that will compute the frequency of a word in each given chapters. we will check the function and its trend in the words `Mody`, `Ahab`, and `sea`.

```{r}
Q2a <- function(word_query, chapters_num){
  relative_freq <- c()
  counter <- 1
  for (i in as.numeric(chapters_num)){
    relative_freq[counter] <- (str_count(chapters[i], word_query)/y_words_per_chapter[i])
    counter <- counter + 1
  }
return(relative_freq)
}

ahab_plot <- Q2a("Ahab", c(1:137))
barplot(ahab_plot, main = "Freq VS. Chapter for Ahab",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkblue")
moby_plot <- Q2a("Moby", c(1:137))
barplot(moby_plot, main = "Freq VS. Chapter for Moby",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkgreen")
sea_plot <- Q2a("sea", c(1:137))
barplot(sea_plot, main = "Freq VS. Chapter for sea",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "yellow")
```

As we can see, the most frequent word is `sea`, which also make sense based on the book's subject. the least frequent word is `Moby`, who barely shows in the book. For `Ahab`, we hear about him in the beginning of the book, and at the end more frequently.

###4.  
###a
```{r, cache=TRUE}
#clean data
clean_words <- words_html[-str_which(words_html,"[^a-z, ]+")] 
clean_five_letter <-str_subset(clean_words, "^.....$")
five_letter_little<- str_to_lower(clean_five_letter)
five_letter_tab <- table(five_letter_little)
#most frequent five-letter words with their frequencies
sort(five_letter_tab, decreasing = TRUE)[1:10]
five_letter_little_2 <- five_letter_little
#The difference between all the words and 5-word words
unique_all_word <-unique(clean_words)
unique_five_words <-unique(five_letter_little)
Number_dif <- length(unique_all_word)-length(unique_five_words)
Number_dif
```
###b
```{r}
freq_table <- function(str) {
  #we creat matrix that help us to see the freqy prsent 
  freq_mat <- matrix(0,26,5)
  colnames(freq_mat) <- c("1","2","3","4","5")
  rownames(freq_mat) <- letters[1:26]
  sum <- 1
#i -Go through every word
      #Go through each letter
  for (i in str){
    for (j in unlist(strsplit(i,""))){
       freq_mat[j,sum] <-  freq_mat[j,sum] + 1
       sum <- sum + 1
      if (sum == 6){sum <- 1}}}
  freq_mat <-freq_mat/ colSums(freq_mat)[1]
  return( freq_mat)}

freq_table_var <-freq_table(unique_five_words)
freq_table_var

#crest heatmap
heatmap(freq_table_var,Colv = NA ,Rowv = NA,scale ="column",xlab = "column",ylab = "letter")
```
###c
```{r}
#Here, too, we use a matrix calculation in order to show the top-10 words with the highest likelihood.
likelihood_words<- function(str) {
  tab_basis<- freq_table(str)
  #normalized to probabilities
  column_a <- as.matrix(str)
  new_word_matrix <- rep(1,length(str))
  new_word_matrix <- cbind(column_a ,new_word_matrix)
  #Create the subject of the columns
  colnames(new_word_matrix) <- c("word", "prob")
  rownames(new_word_matrix) <- c(1:length(str))
  new_word_matrix <- as.data.frame(new_word_matrix)
  new_word_matrix[,2] <- as.numeric(new_word_matrix[,2])
  #i -Go through every word
  for (i in 1:length(str)) {
    worde_2 <- unlist(strsplit(str[i],""))
    #Go through each letter
    for (j in 1:length(worde_2)){
        new_word_matrix[i,2] <- new_word_matrix[i,2] * tab_basis[worde_2[j],j]}}
  #Show the top-10 words with the highest likelihood
  order_words <- new_word_matrix[order(-new_word_matrix$prob),]
  return(order_words)}
tab_likelihood<-likelihood_words(unique_five_words)
head(tab_likelihood,10)
```

###5
###A
```{r}
#read list
url2 <- 'https://raw.githubusercontent.com/DataScienceHU/Lab_2_Cohen_Halit_and_Mauda_Yoray/main/sgb-words.txt'
# Read the html into r:
w_words <- read_html(url2)
w_words_5 <- w_words %>%html_text()
w_words_5 <-w_words_5%>% str_split("\n")%>%unlist()
class(w_words_5)
```

```{r}
# frequency table: 
letter_freq_ww5 <-w_words_5 %>% str_split("") %>% # Split to a vector 
  map(unique) %>%unlist() %>% #one letter per word
  table() %>% sort(decreasing = TRUE) %>%
  `/`(length(w_words_5)) %>% c()

#frequency table per position:
freq_table(w_words_5) 
```

```{r}
length(w_words_5)
length(unique_five_words)
```
It can be seen that because the number of words in the new vocabulary is greater than the number of words we extracted from the book the probabilities change.  
There are places where the probability of the signal being in place has increased, a prominent example of this can be seen in the letter Z in position 5.  
In addition there are places where the probability has decreased and there are places that remain unchanged such as the presence of the letter i at the end of a word.  

###b
```{r}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}
```

```{r}
mach_word<- function(guess,results,dictionary){
  words_d <- dictionary
  #i -Go through every word 
  #j -Go through every Single result
  for (i in dictionary){
    for (j in 1:length(results)){
      match <- wordle_match(guess[[j]], i)
      word_match_test <-match == results[[j]]
      #We will leave only what had a match
      if (length(word_match_test[word_match_test == TRUE]) != 5) {
        words_d <- words_d[words_d != i]}}} 
  return(words_d)}
#chek the function
gues<- list("south", "north")
res <- list(c(-1, 1, 1, 0, 0), c(0, 1, 0, 0, 0))
mach_word(gues,res,w_words_5)
```