---
title: "52414 - lab 2"
author: "Yoray Mauda 315874404, Halit Cohen 318356854"
date: "21/06/2022"
output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```  

# Solutions:

## Part 1 - MOBY DICK

### Question 1

In this question, we will wrangle and scrape data from the book "Moby Dick", extract information, print text, make numerous manipulation on the text, and also create different distributions for it.

###### A

First, we will fetch the HTML, and then we will print the first line.

```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
# Read the html into r:
webpage <- read_html(url)
```  

```{r}
# In line 35, We are creating the first paragraph, and then extract the first line out of it. In line 36 we're returning the sentence.
first_sentence <- webpage %>% html_nodes("div") %>% html_text()
first_sentence[1]
```

###### B

```{r}
words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
words_html <- words_html[[1]]
words_html <- words_html[words_html!=""]
# Counting words
print(length(words_html))
# Length words
words_n <- nchar(words_html)
# Longest word, mean, and median
max(words_n)
mean(words_n)
median(words_n)
# Create frequency table
freq_words <- sort(table(unlist(words_html)),
               decreasing = TRUE)
# Most common word
mcw <- which.max(freq_words)
mcw
# the distribution of word 
tab_n <- table(str_length(words_html))/length(words_html)
barplot(tab_n,main="Word Length Distribution")
```

Not surprisingly, the most common words are conjunction or body pronouns.  

### Question 2

###### A

In this section we will divide the text into chapters, and will plot the count of the number of words per each episode.

```{r}
moby_text <- html_text2(webpage)
```

To discard the beginning until the EXTRACT section, We will use `str_split` and remove the beginning.

```{r}
cut_text <- str_split(moby_text, "\rErromangoan.\t\r\n\r H2 anchor\r\n\r\n\n\n\n\n\r\n\r ")[[1]][2]
```

We will not print the results we have received because it is long, but we have notices that between chapters we have division that looks like `\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r `. Thus, we will split our text based on this division.

```{r}
chapters <- strsplit(cut_text, "\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r ")
chapters <- chapters[[1]]
chapters <- str_to_lower(chapters)
chapters <- gsub('[[:punct:]]+',"",chapters)
```

Now, for plotting the data, we will loop over the chapters and extract the wanted data for our plot

```{r}
  x_chapter_num <- c()
  y_words_per_chapter <- c()
  for (i in 1:length(chapters)){
    fixed_chapter_i <- str_replace_all(chapters[i], "\\s+", " ")
    splitsplat <- str_split(fixed_chapter_i, " ")
    splitsplat <- c(splitsplat)
    splitsplat <- splitsplat[[1]]
    x_chapter_num[i] <- i
    y_words_per_chapter[i] <- length(splitsplat)
  }
barplot(y_words_per_chapter, main = "Number of Words Per Chapter",
        xlab = "Chapter Number", ylab = "Words Number", names.arg = x_chapter_num,
        col = "darkred")
```

###### B

For this section we will create a function that will compute the frequency of a word in each given chapters. we will check the function and its trend in the words `Mody`, `Ahab`, and `sea`.

```{r}
Q2a <- function(word_query, chapters_num){
  word_query <- str_to_lower(word_query)
  relative_freq <- c()
  counter <- 1
  for (i in as.numeric(chapters_num)){
    relative_freq[counter] <- (str_count(chapters[i], word_query)/y_words_per_chapter[i])
    counter <- counter + 1
  }
return(relative_freq)
}

ahab_plot <- Q2a("Ahab", c(1:137))
barplot(ahab_plot, main = "Freq VS. Chapter for Ahab",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkblue")
moby_plot <- Q2a("Moby", c(1:137))
barplot(moby_plot, main = "Freq VS. Chapter for Moby",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkgreen")
sea_plot <- Q2a("sea", c(1:137))
barplot(sea_plot, main = "Freq VS. Chapter for sea",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "yellow")
```

As we can see, the most frequent word is `sea`, which also make sense based on the book's subject. the least frequent word is `Moby`, who barely shows in the book. For `Ahab`, we hear about him in the beginning of the book, and at the end more frequently.

### Question 3

###### A

Suppose we have a fair dice, with 6 numbers on each side. Let's say Halit and I roll the 
dice. We know that the probability to receive a single number is $\frac{1}{6}$. Thus, it does not matter the outcome, the probability for me to choose the same number as Halit is $ P = (\frac{1}{6})^2$. This example was given in order to show that for Alice and Bob to choose the same word, is finding out the number of times a word is in the whole book, then finding out how many words we have in general, and finally just multiply this result by itself. Let's begin. First, we have the vector `y_words_per_chapter` from previous missions, so lets sum them all up to find out how many words we have overall.

```{r}
total_words_amount <- sum(y_words_per_chapter)
total_words_amount
```

After finding out the total number of words we have, we would like to check how many number of a specific word we have inside a book. For that, we will take the OG `Q2a` function and will change it a bit. <br> for convenience, we will create a full list of book string and unique string, and then we will continue. Plus, we will create a comfortable data frame, and we will add freq to it later.

```{r}
full_book_string <- webpage %>% html_nodes("body") %>% html_text()
full_book_string <- str_replace_all(full_book_string, "\\s+", " ")
full_book_string <- str_to_lower(full_book_string)
full_book_string <- gsub('[[:punct:]]+',"",full_book_string)
full_book_string <- gsub('[[:digit:]]+', "", full_book_string)
full_book_string <- str_split(full_book_string, " ")
full_book_string <- full_book_string[[1]]
full_book_string <- str_subset(full_book_string, ".+")
full_book_df <- data.frame(word = full_book_string)
words_data <- full_book_df %>% group_by(word) %>% count()
```

Now let us check everything, starting with the wanted function for a specific word.

```{r}
Q3a <- function(word_query){
  word_query <- str_to_lower(word_query)
  total_word_count <- c()
  for (i in 1:137){
    total_word_count[i] <- str_count(chapters[i], word_query)
  }
  freq = sum(total_word_count)/total_words_amount
  return (freq^2)
}
```

Before, we create and manipulated different data frames, now we will create a frequency column for it, and we can see everything together.

```{r}
words_data$frequency <- words_data$n/total_words_amount
```
We have `Alice` and `Bob`, so it is squared. Let's us create the squared results as well, and then we will sum in all up to see the result.

```{r}
words_data$squared_freq <- (words_data$frequency)^2
sum(words_data$squared_freq)
```

After computing the freq of every word in the book, we can finally compute to probability. For the second method, we will make 100,000 rounds of simulation to see if `Alice` and `Bob` choose the same word.

```{r}
bob_alice_same_word <- 0
for (i in 1:100000){
event1 <- sample(full_book_string, 1)
event2 <- sample(full_book_string, 1)
if (event1 == event2){
  bob_alice_same_word <- bob_alice_same_word + 1}}
freq_bob_alice <- bob_alice_same_word/100000
```

We can see the difference is `r abs(sum(words_data$squared_freq) - freq_bob_alice)`, which is very small difference. Thus, we can conclude it is the same.

###### B

Lets create a string with only unique words first. <br> Then, we will compute the probability in that case, and compare it to our first initial thought.

```{r}
unique_full_book <- unique(full_book_string)
```

Our initial `full_book_string` have `r length(full_book_string)` words, and our `unique_full_book` have `r length(unique_full_book)` words. <br> Just like in the dice example, when we have unique numbers all the time, we can do the same here, and compute it as one over length of `unique_full_book`. the result is `r (1/length(unique_full_book))^2`. the probability is extremely smaller than the others. So yeah, it is smaller.

### Question 4

###### A

```{r}
# Clean data
clean_words <- words_html[-str_which(words_html,"[^a-z, ]+")] 
clean_five_letter <-str_subset(clean_words, "^.....$")
five_letter_little<- str_to_lower(clean_five_letter)
five_letter_tab <- table(five_letter_little)
# Most frequent five-letter words with their frequencies
sort(five_letter_tab, decreasing = TRUE)[1:10]
five_letter_little_2 <- five_letter_little
# The difference between all the words and 5-word words
unique_all_word <-unique(clean_words)
unique_five_words <-unique(five_letter_little)
number_dif <- length(unique_all_word)-length(unique_five_words)
number_dif
```

###### B

```{r}
freq_table <- function(str) {
  # We create matrix that help us to see the freq present 
  freq_mat <- matrix(0,26,5)
  colnames(freq_mat) <- c("1","2","3","4","5")
  rownames(freq_mat) <- letters[1:26]
  sum <- 1
#i -Go through every word
      #Go through each letter
  for (i in str){
    for (j in unlist(strsplit(i,""))){
       freq_mat[j,sum] <-  freq_mat[j,sum] + 1
       sum <- sum + 1
      if (sum == 6){sum <- 1}}}
  freq_mat <-freq_mat/ colSums(freq_mat)[1]
  return( freq_mat)}

freq_table_var <-freq_table(unique_five_words)
freq_table_var

# Create heat map
heatmap(freq_table_var,Colv = NA ,Rowv = NA,scale ="column",xlab = "column",ylab = "letter")
```

We can see that ‘e','S’ are most common and the place that "s" most common is in the first and end the word.


###### C

```{r}
# Here, too, we use a matrix calculation in order to show the top-10 words with the highest likelihood.
likelihood_words<- function(str) {
  tab_basis<- freq_table(str)
  # Normalized to probabilities
  column_a <- as.matrix(str)
  new_word_matrix <- rep(1,length(str))
  new_word_matrix <- cbind(column_a ,new_word_matrix)
  # Create the subject of the columns
  colnames(new_word_matrix) <- c("word", "prob")
  rownames(new_word_matrix) <- c(1:length(str))
  new_word_matrix <- as.data.frame(new_word_matrix)
  new_word_matrix[,2] <- as.numeric(new_word_matrix[,2])
  # i - Goes through every word
  for (i in 1:length(str)) {
    worde_2 <- unlist(strsplit(str[i],""))
    # Goes through each letter
    for (j in 1:length(worde_2)){
        new_word_matrix[i,2] <- new_word_matrix[i,2] * tab_basis[worde_2[j],j]}}
  # Shows the top-10 words with the highest likelihood
  order_words <- new_word_matrix[order(-new_word_matrix$prob),]
  return(order_words)}
tab_likelihood<-likelihood_words(unique_five_words)
head(tab_likelihood,10)
```

### Question 5

###### A

```{r}
#read list
url2 <- 'https://raw.githubusercontent.com/DataScienceHU/Lab_2_Cohen_Halit_and_Mauda_Yoray/main/sgb-words.txt'
# Read the html into r:
w_words <- read_html(url2)
w_words_5 <- w_words %>% html_text()
w_words_5 <- w_words_5 %>% str_split("\n") %>% unlist()
class(w_words_5)
```

```{r}
# frequency table: 
letter_freq_ww5 <-w_words_5 %>% str_split("") %>% # Split to a vector 
  map(unique) %>%unlist() %>% #one letter per word
  table() %>% sort(decreasing = TRUE) %>%
  `/`(length(w_words_5)) %>% c()

#frequency table per position:
freq_table_v<-freq_table(w_words_5)
freq_table_v
heatmap(freq_table_v,Colv = NA ,Rowv = NA,scale ="column",xlab = "column",ylab = "letter")

```

```{r}
length(w_words_5)
length(unique_five_words)
```
It can be seen that because the number of words in the new vocabulary is greater than the number of words we extracted from the book the probabilities change.  
There are places where the probability of the signal being in place has increased, a prominent example of this can be seen in the letter Z in position 5.  
In addition there are places where the probability has decreased and there are places that remain unchanged such as the presence of the letter i at the end of a word.  

###### B

```{r}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}
```

Now let's use the helper function:

```{r}
mach_word<- function(guess,results,dictionary){
  words_d <- dictionary
  #i -Go through every word 
  #j -Go through every Single result
  for (i in dictionary){
    for (j in 1:length(results)){
      match <- wordle_match(guess[[j]], i)
      word_match_test <- match == results[[j]]
      #We will leave only what had a match
      if (length(word_match_test[word_match_test == TRUE]) != 5) {
        words_d <- words_d[words_d != i]}}} 
  return(words_d)}
#chek the function
gues<- list("south", "north")
res <- list(c(-1, 1, 1, 0, 0), c(0, 1, 0, 0, 0))
mach_word(gues,res,w_words_5)
```

### Question 6

###### A

In this section & question, we will solve our given word in a naive way. We will create a function that will receive the word we want to guess, and will first sample a random combination of words. the random combination will check itself against the given word we want to guess. The letters in the correct places will remain, and the rest of the place will get random again. <br> At the end, we will have the correct word after all the iterations, and we will also return the number of iterations it took us. Let's start.

```{r}
strategy_1 <- function(unknown_word){
  unknown_word <- str_to_lower(unknown_word)
  added <- c()
  condition <- FALSE
  counter <- 0
  while (condition == FALSE){
    comb <- sample(letters, 5)
    comb <- paste(comb, collapse = "")
    match <- wordle_match(comb, unknown_word)
    split_comb <- str_split(comb, "")
    split_comb <- split_comb[[1]]
    added[which(match == 1)] <- split_comb[which(match == 1)]
    added2 <- paste(added, collapse = "")
    if (added2 != unknown_word) {
      counter <- counter + 1
    } else {
      condition <- TRUE
    }
  }
  return(counter)
}
```

We have created the function. Now let's see the naive way on the word `mouse`.

```{r}
test <- strategy_1("mouse")
test
```

###### B

In this section, we will write a mathematical formula for the distribution of the number of turns needed to guess the target word with the strategy in sub section A. Let $X$ be the five letter word we are trying to compute, and $P$ is the probability. We know that each letter of $X$ are geometrically distributed, since we are trying to match a letter randomly until we succeed. $X_{i} , 1 \le i \le 5$. We know that the *cumulative* geometric distribution is $P(X\le x) = 1 - (1-p)^x$. We are not using the regular probability distribution from obvious reason, which is we can't use it because we have $X_{i}$ every time for each letter inside of $X$, and we continue to do it inside until we finish everything in $X$. In order to find specific place in time, and to find out what $P(X=k), 1 \le k \le \infty$, we can understand that we can take all the cumulative until that point, and reduce it from everything until the point before. And since $K\in \mathbb{N}$, it makes since. The mathematical notation is $P(X=k)=P(X \le k)-P(X \le k-1)$. <br> If we will input the cumulative from before we receive: $$P(X=k)=1 - (1-p)^k -[1 - (1-p)^{k-1}]$$ We know that $P$ is the probability to nail a letter correct out of the English letters, so it is $\frac{1}{26}$. So obviously $1-P=\frac{25}{26}$. <br> inserting the data we receive $P(X=k)=1 - (\frac{25}{26})^k -[1 - (\frac{25}{26})^{k-1}]$. We now have the specific distribution, but in order to have it across all the indexes, we need to raise it to the power of 5. We were asked to find the number of turns needed to guess the target word with this strategy. From the question, we can conclude we are talking about $E(X)$. The final product is $$E(X) = \sum_{k=1}^{\infty} k*\left[ [1 - (\frac{25}{26})^k]^5 -[1 - (\frac{25}{26})^{k-1}]^5 \right]$$ $Q.E.D$ <br> Now, let's run for 10,000 tries and see what we get for $E(X)$.

```{r}
E <- 0 # The expectancy
E_v <- c()
probability <- 0
cumulative_prob <- c()
for (k in 1:10000){
  E_v[k] <- ((1-(25/26)**k)**5 - (1-(25/26)**(k-1))**5)
  E <- E + k*E_v[k]
  probability <- probability + E_v[k]
  cumulative_prob[k] <- probability
}
E
```

We received $E(X)$ is equal to `r E`.

###### C

In this sub section, we will compute empirically the distribution of the number of turns using the following Monte-Carlo simulation we learned in class.

```{r}
# 1 - Drawing 5 letters from our list of 5 letter words
random_100 <- sample(unique_five_words, 100)
num_of_tries <- c()
# 2 - For each unknown word, we run the guessing strategy implemented in (a.) and record the number of turns
for (i in 1:length(random_100)){
  num_of_tries[i] <- strategy_1(random_100[i])
}
# Step 3 - Computing the AVG
avg_number_of_tries <- sum(num_of_tries)/100
```

We received the average number of tries is `r avg_number_of_tries`. Now let's prepare step 3 - plotting everything together.

```{r}
plot(seq(1,max(num_of_tries)),cumulative_prob[1:max(num_of_tries)], xlab = 'Num of Tries', ylab = 'Probability')
lines(ecdf(num_of_tries), col= 'yellow')
```

From the plot, we can see that we are pretty close according to Monte-Carlo way we learned. For Expectancy comparison, we have `r E` in the first method, and here we received `r sum(num_of_tries)/100`. We can see the difference is `r abs(E - sum(num_of_tries)/100)`, which is not a big of a difference.

### Question 7

###### A

In this sub question we will implement 2 new strategies: *strategy 3* and *strategy 2*. <br> For *strategy 2*, we will guess the word with the highest likelihood, of the remaining words that are consistent with the previous guesses, at each stage. That means, we keep guessing until we find the correct word. <br> we will call the function as instructed, `strategy_2`.
```{r}
strategy_2 <- function(given_word){
  word_list = unique_five_words
  count <- 0
  
# make freq function
  likelyhood_list_ordered <- likelihood_words(word_list)
  
  guess_word <- likelyhood_list_ordered$word[1]
  result <- wordle_match(guess_word, given_word)
  while (sum(result) != 5){
    count <- count + 1
    
    word_list <- mach_word(guess_word, result,word_list)
    
    likelyhood_list_ordered <- likelihood_words(word_list)
    guess_word <- likelyhood_list_ordered$word[1]
    result <- wordle_match(guess_word, given_word)
  }
return(count)
}
```

`strategy_3`
```{r}
```

###B.  
the empirical CDFs of the number of guesses with  100 simulations show as :the mean and the similar between  the  strategy when we play the game.
```{r}
Q.7.b <- function(strategy_2,strategy_3){
   #  take 100 words random in randome
  word_unknow <- unique_five_words[sample(1:length(unique_five_words), 1000, TRUE)]
  #df_strategy_2
  rec2 <- sapply(word_unknow, strategy_2)
  ave2 <- mean(rec2)
  ecdf2 <- my_ecdf(1:25, sort(rec2))
  df2 = data.frame(x=1:25, CDF=CDF_2, type="strategy_2")
  
  #df_strategy_3
  rec3 <- sapply(word_unknow, strategy_3) 
  ave3 <- mean(rec3)
  ecdf3 <- my_ecdf(1:20, sort(rec3))
  df3 = data.frame(x=1:20, CDF=CDF_3, type="strategy_3")
  
  #union
  df_ecdf <- rbind(df2,df3)
  
  ggplot(df_ecdf) +
    geom_line(aes(x, CDF, colour=type))
  }
```

