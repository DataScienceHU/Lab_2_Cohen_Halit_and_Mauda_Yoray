---
title: "52414 - lab 2 "
author: "Yoray Mauda 315874404 and Halit Cohen 318356854"
date: "15/06/2022"
output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```  

## Part 1 - MOBY DICK

### Question 1

In this question, we will wrangle and scrape data from the book "Moby Dick", extract information, print text, make numerous manipulation on the text, and also create different distributions for it.

###### A

First, we will fetch the HTML, and then we will print the first line.

```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
# Read the html into r:
webpage <- read_html(url)
```  

```{r}
# In line 35, We are creating the first paragraph, and then extract the first line out of it. In line 36 we're returning the sentence.
first_sentence <- webpage %>% html_nodes("div") %>% html_text()
first_sentence[1]
```

###### B

```{r}
words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
words_html <- words_html[[1]]
words_html <- words_html[words_html!=""]
#count words
print(length(words_html))
# length words
words_n <-nchar(words_html)
#longest word, mean, median
max(words_n)
mean(words_n)
median(words_n)
# Create frequency table
freq_words <- sort(table(unlist(words_html)),
               decreasing = TRUE)
#most common word
mcw <-which.max(freq_words)
mcw
# the distribution of word 
tab_n<-table(str_length(words_html))/length(words_html)
barplot(tab_n,main="Word Length Distribution")
```

### Question 2

###### A

In this section we will divide the text into chapters, and will plot the count of the number of words per each episode.

```{r}
moby_text <- html_text2(webpage)
```

To discard the beginning until the EXTRACT section, We will use `str_split` and remove the beginning.

```{r}
cut_text <- str_split(moby_text, "\rErromangoan.\t\r\n\r H2 anchor\r\n\r\n\n\n\n\n\r\n\r ")[[1]][2]
```

We will not print the results we have received because it is long, but we have notices that between chapters we have division that looks like `\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r `. Thus, we will split our text based on this division.

```{r}
chapters <- strsplit(cut_text, "\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r ")
chapters <- chapters[[1]]
chapters <- str_to_lower(chapters)
chapters <- gsub('[[:punct:]]+',"",chapters)
```

Now, for plotting the data, we will loop over the chapters and extract the wanted data for our plot

```{r}
  x_chapter_num <- c()
  y_words_per_chapter <- c()
  for (i in 1:length(chapters)){
    fixed_chapter_i <- str_replace_all(chapters[i], "\\s+", " ")
    splitsplat <- str_split(fixed_chapter_i, " ")
    splitsplat <- c(splitsplat)
    splitsplat <- splitsplat[[1]]
    x_chapter_num[i] <- i
    y_words_per_chapter[i] <- length(splitsplat)
  }
barplot(y_words_per_chapter, main = "Number of Words Per Chapter",
        xlab = "Chapter Number", ylab = "Words Number", names.arg = x_chapter_num,
        col = "darkred")
```

###### B

For this section we will create a function that will compute the frequency of a word in each given chapters. we will check the function and its trend in the words `Mody`, `Ahab`, and `sea`.

```{r}
Q2a <- function(word_query, chapters_num){
  word_query <- str_to_lower(word_query)
  relative_freq <- c()
  counter <- 1
  for (i in as.numeric(chapters_num)){
    relative_freq[counter] <- (str_count(chapters[i], word_query)/y_words_per_chapter[i])
    counter <- counter + 1
  }
return(relative_freq)
}

ahab_plot <- Q2a("Ahab", c(1:137))
barplot(ahab_plot, main = "Freq VS. Chapter for Ahab",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkblue")
moby_plot <- Q2a("Moby", c(1:137))
barplot(moby_plot, main = "Freq VS. Chapter for Moby",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkgreen")
sea_plot <- Q2a("sea", c(1:137))
barplot(sea_plot, main = "Freq VS. Chapter for sea",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "yellow")
```

As we can see, the most frequent word is `sea`, which also make sense based on the book's subject. the least frequent word is `Moby`, who barely shows in the book. For `Ahab`, we hear about him in the beginning of the book, and at the end more frequently.

### Question 3

###### A

Suppose we have a fair dice, with 6 numbers on each side. Let's say Halit and I roll the 
dice. We know that the probability to receive a single number is $\frac{1}{6}$. Thus, it does not matter the outcome, the probability for me to choose the same number as Halit is $ P = (\frac{1}{6})^2$. This example was given in order to show that for Alice and Bob to choose the same word, is finding out the number of times a word is in the whole book, then finding out how many words we have in general, and finally just multiply this result by itself. Let's begin. First, we have the vector `y_words_per_chapter` from previous missions, so lets sum them all up to find out how many words we have overall.

```{r}
total_words_amount <- sum(y_words_per_chapter)
total_words_amount
```

After finding out the total number of words we have, we would like to check how many number of a specific word we have inside a book. For that, we will take the OG `Q2a` function and will change it a bit.

```{r}
Q3a <- function(word_query){
  word_query <- str_to_lower(word_query)
  total_word_count <- c()
  for (i in 1:137){
    total_word_count[i] <- str_count(chapters[i], word_query)
  }
  return (sum(total_word_count))
}
```

After computing number of given word in the book, we can finally compute to probability. For example purposes, lets say we want the word `sea`. So we will search the word, divide it in the total number of word in book, and because we hate two independent experiments we will raise it to the power of 2. We will receive `r (Q3a("sea")/total_words_amount)^2`, which is a very small probability indeed. <br> For the second method, we will make 100,000 rounds of simulation to see. But first, we will create an entire string of the book for the job.

```{r}
full_book_string <- webpage %>% html_nodes("body") %>% html_text()
full_book_string <- str_replace_all(full_book_string, "\\s+", " ")
full_book_string <- str_to_lower(full_book_string)
full_book_string <- gsub('[[:punct:]]+',"",full_book_string)
full_book_string <- gsub('[[:digit:]]+', "", full_book_string)
full_book_string <- str_split(full_book_string, " ")
full_book_string <- full_book_string[[1]]
full_book_string <- str_subset(full_book_string, ".+")
```

Now that we finally have a divided long, cleaned string of the book, we can start to simulate.

```{r}
B <- 100000
events <- replicate(B, sample(full_book_string, 1))
tab <- table(events)
prop_table <- prop.table(tab) # We are not printing it because it is a very large long table we do not want to show.
```

We can now see that the odds for each word, are very small. just like our first examination. <br> Most words in the book are pretty rare, and not commonly used. In addition, we can look at the table and see that most of the proportions are $0.00001$, which is indeed very small. Raise it to the power of 2 and you will receive `r 0.00001^2`. Very small! we can conclude that for the minimum of words occurrences, we do have a very small chance in having `Alice` and `Bob` choose the same word.

###### B

Lets create a string with only unique words first. <br> Then, we will compute the probability in that case, and compare it to our first initial thought.

```{r}
full_unique_book_string <- unique(full_book_string)
```

Our initial `full_book_string` had `r length(full_book_string)` words, and our `full_unique_book_string` has `r length(full_unique_book_string)` words. Now let us check the probability. <br> We know that in `full_unique_book_string` we have each word only one time. so, the probability to choose each word is $\frac{1}{uniquetotalnum}$, raised to the power of 2. Thus, we can see that the probability is `r (1/19966)^2`. <br> we have even smaller odds now, and it makes sense. when we have such a major proportion difference, it is logical that we have a smaller chance. Thus, we can conclude that the probability in a is *higher*.

### Question 4

###### A

```{r}
#clean data
clean_words <- words_html[-str_which(words_html,"[^a-z, ]+")] 
clean_five_letter <-str_subset(clean_words, "^.....$")
five_letter_little<- str_to_lower(clean_five_letter)
five_letter_tab <- table(five_letter_little)
#most frequent five-letter words with their frequencies
sort(five_letter_tab, decreasing = TRUE)[1:10]
five_letter_little_2 <- five_letter_little
#The difference between all the words and 5-word words
unique_all_word <-unique(clean_words)
unique_five_words <-unique(five_letter_little)
number_dif <- length(unique_all_word)-length(unique_five_words)
number_dif
```

###### B

```{r}
letter_freq <-five_letter_little_2 %>% str_split("") %>% # Split  word to a vector
  map(unique) %>%  # Keep one of each letter per word

  # Unlist into a big vector of letters
  unlist() %>%
  # Count the letters (each appearance in a word)
  table() %>%
  # Most popular letters first
  sort(decreasing = TRUE) %>%
  # Turn into frequency table
  `/`(length(five_letter_little_2)) %>%
  # Remove attributes from table()
  c()
letter_freq_pos <-
  tibble(word = five_letter_little_2) %>%
  select(word) %>%
  mutate(letter = word) %>%
  tidyr::separate_rows(letter, sep = "") %>%
  filter(letter != "") %>%
  group_by(word) %>%
  mutate(position = row_number()) %>%
  group_by(letter, position) %>%
  summarize(n = n(), .groups = "drop") %>%
  mutate(
    five_letter_little_2 = letter_freq[letter] * length(!!five_letter_little_2),
    freq = n / five_letter_little_2
  ) %>%
  select(-n, -five_letter_little_2) %>%
  tidyr::pivot_wider(
    names_from = position,
    values_from = freq,
    values_fill = 0,
    names_prefix = "p"
  )
letter_freq_pos
```

## PART 2 - WORDLE

```{r, cache=TRUE, include=FALSE}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}
```

### Question 5

###### A

### Question 6

###### A

In this section & question, we will solve our given word in a naive way. We will create a function that will receive the word we want to guess, and will first sample a random combination of words. the random combination will check itself against the given word we want to guess. The letters in the correct places will remain, and the rest of the place will get random again. <br> At the end, we will have the correct word after all the iterations, and we will also return the number of iterations it took us. Let's start.

```{r}
strategy_1 <- function(unknown_word){
  unknown_word <- str_to_lower(unknown_word)
  added <- c()
  condition <- FALSE
  counter <- 0
  while (condition == FALSE){
    comb <- sample(letters, 5)
    comb <- paste(comb, collapse = "")
    match <- wordle_match(comb, unknown_word)
    split_comb <- str_split(comb, "")
    split_comb <- split_comb[[1]]
    added[which(match == 1)] <- split_comb[which(match == 1)]
    added2 <- paste(added, collapse = "")
    if (added2 != unknown_word) {
      counter <- counter + 1
    } else {
      print("Success!")
      condition <- TRUE
    }
  }
  return(counter)
}
```

We have created the function. Now let's see the naive way on the word `mouse`.

```{r}
test <- strategy_1("mouse")
test
```

###### B

In this section, we will write a mathematical formula for the distribution of the number of turns needed to guess the target word with the strategy in sub section A.





