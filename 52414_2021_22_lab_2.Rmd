---
title: "52414 - lab 2 "
author: "Yoray Mauda 315874404 and Halit Cohen 318356854"
date: "09/06/2022"
output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```  

### Question 1

In this question, we will wrangle and scrape data from the book "Moby Dick", extract information, print text, make numerous manipulation on the text, and also create different distributions for it.

###### A

First, we will fetch the HTML, and then we will print the first line.

```{r, include=FALSE}
url <- 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'
# Read the html into r:
webpage <- read_html(url)
```  

```{r}
# In line 35, We are creating the first paragraph, and then extract the first line out of it. In line 36 we're returning the sentence.
first_sentence <- webpage %>% html_nodes("div") %>% html_text()
first_sentence[1]
```

###### B

```{r}
words_html <- webpage %>% html_text() %>% str_split(",|\\.|[:space:]")
words_html <- words_html[[1]]
words_html <- words_html[words_html!=""]
#count words
print(length(words_html))
# length words
words_n <-nchar(words_html)
#longest word, mean, median
max(words_n)
mean(words_n)
median(words_n)
# Create frequency table
freq_words <- sort(table(unlist(words_html)),
               decreasing = TRUE)
#most common word
mcw <-which.max(freq_words)
mcw
# the distribution of word 
len_words <- length(words_html)
vec_word_n <- c(rep(0,length(words_html)))
for (i in (1:length(words_html))) {
  vec_word_n[i] = (words_n[i]/len_words)
}
tab_wn <- table(words_n,vec_word_n)
df <-data.frame(tab_wn)
barplot(tab_wn, main="word length Distribution",
   xlab="length word")
```

### Question 2

###### A

In this section we will divide the text into chapters, and will plot the count of the number of words per each episode.

```{r}
moby_text <- html_text2(webpage)
```

To discard the beginning until the EXTRACT section, We will use `str_split` and remove the beginning.

```{r}
cut_text <- str_split(moby_text, "\rErromangoan.\t\r\n\r H2 anchor\r\n\r\n\n\n\n\n\r\n\r ")[[1]][2]
```

We will not print the results we have received because it is long, but we have notices that between chapters we have division that looks like `\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r `. Thus, we will split our text based on this division.

```{r}
chapters <- strsplit(cut_text, "\r\n\n\r\n\n\r \r \r\n\n\r\n\r\n\n\n\n\n\r\n\r ")
chapters <- chapters[[1]]
chapters <- str_to_lower(chapters)
chapters <- gsub('[[:punct:]]+',"",chapters)
```

Now, for plotting the data, we will loop over the chapters and extract the wanted data for our plot

```{r}
  x_chapter_num <- c()
  y_words_per_chapter <- c()
  for (i in 1:length(chapters)){
    fixed_chapter_i <- str_replace_all(chapters[i], "\\s+", " ")
    splitsplat <- str_split(fixed_chapter_i, " ")
    splitsplat <- c(splitsplat)
    splitsplat <- splitsplat[[1]]
    x_chapter_num[i] <- i
    y_words_per_chapter[i] <- length(splitsplat)
  }
barplot(y_words_per_chapter, main = "Number of Words Per Chapter",
        xlab = "Chapter Number", ylab = "Words Number", names.arg = x_chapter_num,
        col = "darkred")
```
###### B

For this section we will create a function that will compute the frequency of a word in each given chapters. we will check the function and its trend in the words `Mody`, `Ahab`, and `sea`.

```{r}
Q2a <- function(word_query, chapters_num){
  word_query <- str_to_lower(word_query)
  relative_freq <- c()
  counter <- 1
  for (i in as.numeric(chapters_num)){
    relative_freq[counter] <- (str_count(chapters[i], word_query)/y_words_per_chapter[i])
    counter <- counter + 1
  }
return(relative_freq)
}

ahab_plot <- Q2a("Ahab", c(1:137))
barplot(ahab_plot, main = "Freq VS. Chapter for Ahab",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkblue")
moby_plot <- Q2a("Moby", c(1:137))
barplot(moby_plot, main = "Freq VS. Chapter for Moby",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "darkgreen")
sea_plot <- Q2a("sea", c(1:137))
barplot(sea_plot, main = "Freq VS. Chapter for sea",
        xlab = "Chapter Number", ylab = "Word Freq", names.arg = x_chapter_num,
        col = "yellow")
```

As we can see, the most frequent word is `sea`, which also make sense based on the book's subject. the least frequent word is `Moby`, who barely shows in the book. For `Ahab`, we hear about him in the beginning of the book, and at the end more frequently.

### Question 3

###### A

Suppose we have a fair dice, with 6 numbers on each side. Let's say Halit and I roll the 
dice. We know that the probability to receive a single number is $\frac{1}{6}$. Thus, it does not matter the outcome, the probability for me to choose the same number as Halit is $ P = (\frac{1}{6})^2$. This example was given in order to show that for Alice and Bob to choose the same word, is finding out the number of times a word is in the whole book, then finding out how many words we have in general, and finally just multiply this result by itself. Let's begin. First, we have the vector `y_words_per_chapter` from previous missions, so lets sum them all up to find out how many words we have overall.

```{r}
total_words_amount <- sum(y_words_per_chapter)
total_words_amount
```

After finding out the total number of words we have, we would like to check how many number of a specific word we have inside a book. For that, we will take the OG `Q2a` function and will change it a bit.

```{r}
Q3a <- function(word_query){
  word_query <- str_to_lower(word_query)
  total_word_count <- c()
  for (i in 1:137){
    total_word_count[i] <- str_count(chapters[i], word_query)
  }
  return (sum(total_word_count))
}
length(splitsplat)
```

After computing number of given word in the book, we can finally compute to probability. For example purposes, lets say we want the word `sea`. So we will search the word, divide it in the total number of word in book, and because we hate two independent experiments we will raise it to the power of 2. We will receive `r (Q3a("sea")/total_words_amount)^2`, which is a very small probability indeed. For the second method, we will make 100,000 rounds of simulation to see. Plus, We will create an entire string of the book for the job.

```{r}
full_book_string <- webpage %>% html_nodes("body") %>% html_text()
full_book_string <- str_replace_all(full_book_string, "\\s+", " ")
full_book_string <- str_to_lower(full_book_string)
full_book_string
```

### Question 4

###### A

```{r}
#clean data
clean_words <- words_html[-str_which(words_html,"[^a-z, ]+")] 
clean_five_letter <-str_subset(clean_words, "^.....$")
five_letter_little<- str_to_lower(clean_five_letter)
five_letter_tab <- table(five_letter_little)
#most frequent five-letter words with their frequencies
sort(five_word_rep, decreasing = TRUE)[1:10]
five_letter_little_2 <- five_letter_little
#The difference between all the words and 5-word words
unique_all_word <-unique(clean_words)
unique_five_words <-unique(five_letter_little)
Number_dif <- length(unique_all_word)-length(unique_five_words)
Number_dif
```

###### B

```{r}
letter_freq <-
  five_letter_little_2 %>%
  # Split each word into a vector of letters
  str_split("") %>%
  # Keep one of each letter per word
  map(unique) %>%
  # Unlist into a big vector of letters
  unlist() %>%
  # Count the letters (each appearance in a word)
  table() %>%
  # Most popular letters first
  sort(decreasing = TRUE) %>%
  # Turn into frequency table
  `/`(length(five_letter_little_2)) %>%
  # Remove attributes from table()
  c()
letter_freq_pos <-
  tibble(word = five_letter_little_2) %>%
  select(word) %>%
  mutate(letter = word) %>%
  tidyr::separate_rows(letter, sep = "") %>%
  filter(letter != "") %>%
  group_by(word) %>%
  mutate(position = row_number()) %>%
  group_by(letter, position) %>%
  summarize(n = n(), .groups = "drop") %>%
  mutate(
    five_letter_little_2 = letter_freq[letter] * length(!!five_letter_little_2),
    freq = n / five_letter_little_2
  ) %>%
  select(-n, -five_letter_little_2) %>%
  tidyr::pivot_wider(
    names_from = position,
    values_from = freq,
    values_fill = 0,
    names_prefix = "p"
  )
letter_freq_pos
```


```{r, cache=TRUE}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}
```


